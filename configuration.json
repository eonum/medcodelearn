{
    "all-tokens": "data/pipelinetest/tokenization/all_tokens.csv",
    "all-vectors": "data/pipelinetest/vectorization/vectors.csv",
    "all-vocab": "data/pipelinetest/tokenization/vocab_all.csv",
    "base_folder": "data/pipelinetest/",
    "chop-catalog": "data/2015/chop_codes.csv",
    "chop-tokenizations": "data/pipelinetest/tokenization/chop_codes_tokenized.csv",
    "classifier": "lstm-embedding",
    "code-tokens": "data/pipelinetest/tokenization/all_tokens_by_code.json",
    "code-vectors": "data/pipelinetest/vectorization/all_vectors_by_code.json",
    "demo-variables": [
        "admWeight",
        "hmv",
        "sex",
        "los",
        "ageYears",
        "ageDays",
        "adm-normal",
        "adm-transfer",
        "adm-transfer-short",
        "adm-unknown",
        "sep-normal",
        "sep-dead",
        "sep-doctor",
        "sep-unknown",
        "sep-transfer"
    ],
    "drg-catalog": "data/2015/drgs.csv",
    "drg-tokenizations": "data/pipelinetest/tokenization/drgs_tokenized.csv",
    "icd-catalog": "data/2015/icd_codes.csv",
    "icd-tokenizations": "data/pipelinetest/tokenization/icd_codes_tokenized.csv",
    "lstm-activation": "tanh",
    "lstm-init": "glorot_uniform",
    "lstm-inner-activation": "hard_sigmoid",
    "lstm-inner-init": "orthogonal",
    "lstm-layers": [
        {
            "dropout": 0.10000000,
            "output-size": 64
        }
    ],
    "maxlen": 32,
    "num-cores": 8,
    "num-shuffles": 10,
    "optimizer": "adam",
    "outlayer-init": "he_uniform",
    "shuffle-word2vec-traindata": true,
    "skip-word2vec": true,
    "store-everything": false,
    "training-set": "data/2015/trainingData2015_20151001.csv.small",
    "training-set-drgs": "data/2015/trainingData2015_20151001.csv.small.out",
    "training-set-word2vec": "data/2015/trainingData2015_20151001.csv.last",
    "use-all-tokens-in-embedding": false,
    "use-descriptions": true,
    "use-training-data-for-word2vec": true,
    "use_demographic_tokens": true,
    "word2vec-cbow": true,
    "word2vec-dim-size": 50,
    "word2vec-vocab": "data/pipelinetest/vectorization/vocab.csv",
    "tokenizer-german-split-compound-words" : false
}
